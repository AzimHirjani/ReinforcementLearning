{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Chess\n",
    "\n",
    "States: Positions of the chess pieces on the board\n",
    "\n",
    "Actions: Which piece to move where\n",
    "\n",
    "Rewards: The outcome of the game -1 for loss, 0 for draw, and 1 for win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Black Jack\n",
    "\n",
    "States: What cards you and the dealer have\n",
    "\n",
    "Actions: To hit or stay\n",
    "\n",
    "Rewards: The outcome of the game -1 for loss, 0 for draw, and 1 for win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Elevator\n",
    "\n",
    "To figure out which floor is the best floor for the elevator to wait on when there are no passengers at a specific time of the day.\n",
    "\n",
    "States: Time of the day and the floor the elevator is on\n",
    "\n",
    "Actions: Which floor to go to so basically move up and down\n",
    "\n",
    "Rewards: -1 for each second that someone is waiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP would not be useful in a situation where the number of states is large. For example the game of Go has $3^{361}$, with this many states it would be hard to experience all the actions that can be taken at all the states causing our agent to be inexperienced.\n",
    "\n",
    "MDP would also not be useful when the reward is not numerical or hard to express as number. If the rewards is feedback in a text format then it would be hard to quantify the reward. We would need to create a seperate and accurate process to convert text feedback to a number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right place to draw the line should depend on a clear definition of what we are doing and who our agent is. In the example of driving we can define the actions, agent, and environment many ways, so having a clear understanding of what we are trying to do is important. But in general I think that the line should be drawn so that the actions are defined in such a way that a single action has an effect on the environment and cannot be broken down into smaller actions that also impact the environment.\n",
    "\n",
    "There are many basis in which one location of the line is better than aother. When you have states that are unnecessarily detailed or actions that barely affect the environment we can end up overcomplicating the problem. This is why we might prefer one location over another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  s | a | s'  | r  | p(s', r \\| s, a) |\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| high  | search  |  high  |  $r_{search}$  |  $\\alpha$ |\n",
    "| high  | search  |  low  |  $r_{search}$  |  $1-\\alpha$ |\n",
    "| low  | search  |  high  |  -3  |  $1-\\beta$ |\n",
    "| low  | search  |  low  |  $r_{search}$  |  $\\beta$ |\n",
    "| high  | wait  |  high  |  $r_{wait}$  |  1 |\n",
    "| low  | wait  |  low  |  $r_{wait}$  |  1 |\n",
    "| low  | recharge  |  high  |  0  |  1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{s'\\in{S^+}}\\sum_{r\\in{R}}p(s', r | s, a) = 1 \\text{ for all s} \\in S \\text{ and a} \\in A(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change here is to use the set of all states including the terminal states for s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{T-t-1} R_{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we substitute rewards with -1 upon failure and 0 otherwise\n",
    "\n",
    "$$ = 0 + \\gamma 0 + \\gamma^2 0 + ... + \\gamma^{T-t-1} -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = - \\gamma^{T-t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same return as in the discounted continuing case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot gets +1 for escaping and since we are dealing with a episodic problem the maximum return we can get is 1 if we reach the end of the maze. The problem here is that no matter how long we take as long as we reach the end the max return we can get is 1. The effective way to communicate to the agent that we want it to find the exit as quick as possible is to give a reward of -1 for each state except for the terminal state. This way the agent will want to maximize reward meaning it wants to take the least amound of time steps as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use (3.9) in the textbook with $G_T = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{T=5} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{4} = R_{5} + \\gamma G_5\\\\\\\\\n",
    " = 2 + 0\\\\\\\\\n",
    " = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{3} = R_{4} + \\gamma G_4\\\\\\\\\n",
    " = 3 + 0.5*2\\\\\\\\\n",
    " = 4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{2} = R_{3} + \\gamma G_3\\\\\\\\\n",
    " = 6 + 0.5*4\\\\\\\\\n",
    " = 8$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{1} = R_{2} + \\gamma G_2\\\\\\\\\n",
    " = 2 + 0.5*8\\\\\\\\\n",
    " = 6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{0} = R_{1} + \\gamma G_1\\\\\\\\\n",
    " = -1 + 0.5*6\\\\\\\\\n",
    " = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using (3.9) and (3.10) in the textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{1} = R_{2} + \\gamma G_2\\\\\\\\\n",
    " = 7 + 0.9*(R_3 + \\gamma G_3)\\\\\\\\\n",
    " = 7 + 0.9*(7 + 0.9*G_3)\\\\\\\\\n",
    " = 7 + 0.9*7 + 0.9^2*G_3\\\\\\\\\n",
    " = \\sum_{k=0}^\\infty{7*0.9^k}\\\\\\\\\n",
    " = 7\\sum_{k=0}^\\infty{0.9^k}\\\\\\\\\n",
    " = 7*\\frac{1}{1-0.9}\\\\\\\\\n",
    " = 70$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_0 = R_1 + \\gamma G_1\\\\\\\\\n",
    "= 2 + 0.9*70\\\\\\\\\n",
    "= 2 + 93\\\\\\\\\n",
    "= 65$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{k=0}^{\\infty}\\gamma^k = \\lim_{k->\\infty}{\\gamma^0+\\gamma^1+\\gamma^2+...+\\gamma^k} = \\lim_{k->\n",
    "\\infty}S_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_k = \\gamma^0+\\gamma^1+\\gamma^2+...+\\gamma^k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\gamma S_k = \\gamma^1+\\gamma^2+\\gamma^3+...+\\gamma^{k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_k - \\gamma S_k = \\gamma^0 - \\gamma^{k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_k(1 - \\gamma) = 1 - \\gamma^{k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_k = \\frac{1 - \\gamma^{k+1}}{1 - \\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lim_{k->\\infty}{S_k} = \\lim_{k->\\infty}{\\frac{1 - \\gamma^{k+1}}{1 - \\gamma}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= \\frac{1-0}{1-\\gamma} \\text{ because gamma < 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= \\frac{1}{1-\\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r(s) = E[R_{t+1}|S_t = s] = \\sum_{a \\in A(s)}\\pi (a|s) \\sum_{r \\in R} r\\sum_{s'\\in S}p(s', r | s, a )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\sum_{a\\in A(s)}\\pi(a|s)q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a) = \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi (a|s) \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= \\pi(north|s)p(\\text{north neighbor}, 0|s, north)[0 + \\gamma v_{\\pi}(\\text{north neighbor})] \\\\ + \\pi(south|s)p(\\text{south neighbor}, 0|s, south)[0 + \\gamma v_{\\pi}(\\text{south neighbor})] \\\\ + \\pi(east|s)p(\\text{east neighbor}, 0|s, east)[0 + \\gamma v_{\\pi}(\\text{east neighbor})] \\\\ + \\pi(west|s)p(\\text{west neighbor}, 0|s, west)[0 + \\gamma v_{\\pi}(\\text{west neighbor})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= (0.25)(1)[0 + 0.9*2.3] \\\\ + (0.25)(1)[0 + 0.9*-0.4] \\\\ + (0.25)(1)[0 + 0.9*0.7] \\\\ + (0.25)(1)[0 + 0.9*0.4]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= 0.5175 + -0.09 + 0.1575 + 0.09$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= 0.675$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\approx 0.7 \\text{ because we are accurate to one decimal place}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(R_{t+1}+c)+\\gamma(R_{t+2}+c)+\\gamma^2(R_{t+3}+c)+...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= R_{t+1} + c + \\gamma R_{t+2} + \\gamma c + \\gamma^2 R_{t+3} + \\gamma^2c +...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= G_t + \\sum_{k=0}^{\\infty}\\gamma^kc$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= G_t + c\\sum_{k=0}^{\\infty}\\gamma^k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$= G_t + \\frac{c}{1-\\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t+\\frac{c}{1-\\gamma}|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s] + \\frac{c}{1-\\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=s] +v_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add a constant c to all rewards we can see in the end that the value of the state is expected return given that we are in state s plus the constant $v_c$ which is equal to $\\frac{c}{1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we discussed before in the episodic maze problem the sign of the rewards is important. Having -1 for all rewards except the terminal one is important to get the agent to try to finish the maze as quick as possible. If the constant added to the negative reward turns it positive we can lose the effect that the negative reward has on the agent. If the constant does not change the reward to negative then it would not have any effect and would leave the task unchanged because the negative reward effect is still there and the interval between the constant is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)=\\sum_{s'\\in A(s)}\\sum_{r\\in R}[r+\\gamma \\sum_{a' \\in A(s')}\\pi (a'|s)q_{\\pi}(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 1$$v_{\\pi}(s)=\\mathbb{E}_{\\pi}[q_{\\pi}(s,a)|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 2$$v_{\\pi}(s)=\\sum_{a\\in A(s)}\\pi(s|a)q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 1$$q_{\\pi}(s,a)=\\mathbb{E}[R_{t+1}+\\gamma V_{\\pi}(S_{t+1})|S_t=s,A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 2$$q_{\\pi}(s,a)=\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal state-value function would be $q_*(s,driver)$ everywhere outside the green and $v_{putt}$ inside the green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sand is -3 because -1 using the putter that keeps us in the sand then we use the driver to get out of the sand into the green then we use a putter to get it into the hole. If we are in the green then it is -1 because we just put it in. If we are outside the green and not in the sand then it will be -1 plus where we end up on $q_*(s,driver)$ unless we end up in the green after the put then it is -2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\gamma=0$ then we care about maximizing immediate rewards so the policy $\\pi_{left}$ would be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tell which policy is optimal we need to see what the value of the state we are making the decision in is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi_{left}}(s)=\\mathbb{E}_{\\pi_{left}}[G_t|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi_{left}}(topstate)=\\sum_{i=0}^{\\infty}1\\gamma^{2i} = \\frac{1}{1-\\gamma^2}$$\n",
    "\n",
    "This is because on each even time step we recieve a reward of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi_{right}}(s)=\\mathbb{E}_{\\pi_{right}}[G_t|S_t=s]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi_{right}}(topstate)=\\sum_{i=0}^{\\infty}1\\gamma^{2i+1} =\\gamma\\sum_{i=0}^{\\infty}1\\gamma^{2i}= \\frac{2\\gamma}{1-\\gamma^2}$$\n",
    "\n",
    "This is because on each odd time step we recieve a reward of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\gamma=0.9$ we care more about future rewards and the value of $v_{\\pi_{left}}(topstate)=\\frac{100}{19}$ and $v_{\\pi_{right}}(topstate)=\\frac{180}{19}$ so policy $\\pi_{left}$ would be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\gamma=0.5$ we care more about future rewards and the value of $v_{\\pi_{left}}(topstate)=\\frac{4}{5}$ and $v_{\\pi_{right}}(topstate)=\\frac{4}{5}$ so both policies would be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(h,s)=\\alpha[r_s+\\gamma max(q_*(h,s),q_*(h,w))] + (1-\\alpha)[r_s+\\gamma max(q_*(l,s),q_*(l,w),q_*(l,r))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(h,w)=[r_w+\\gamma max(q_*(h,s),q_*(h,w))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(l,s)=\\beta[-3+\\gamma max(q_*(h,s),q_*(h,w))]+(1-\\beta)[r_s+\\gamma max(q_*(l,s),q_*(l,w),q_*(l,r))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(l,w)=[r_w+\\gamma max(q_*(l,s),q_*(l,w),q_*(l,r))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(l,r)=[0+\\gamma max(q_*(h,s),q_*(h,w))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that $\\gamma=0.9$ and that following the policy after we go from A to A' and get a reward of +10 it will take 5 steps to get this reward again. Using (3.8)\n",
    "\n",
    "$$v_*(A)=\\sum_{i=0}^{\\infty}10\\gamma^{5i}=\\frac{10}{1-\\gamma^5}=24.4194281\\approx24.419$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_*(s)=\\max_{a\\in A(s)}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_*(s,a)=\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r+\\gamma v_*(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_*(s)=arg\\max_{a\\in A(s)}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_*(s)=arg\\max_{a\\in A(s)}\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r+\\gamma v_*(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi (a|s) \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi (a|s) [\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)r + \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)\\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi (a|s) [\\sum_{r\\in R}r\\sum_{s'\\in S}p(s',r|s,a) + \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)\\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "Now we substitute (3.4) and (3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi (a|s) [r(s,a) + \\sum_{s'\\in S}p(s'|s,a)\\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $q_{\\pi}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a) =\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma  \\sum_{a' \\in A(s)}\\pi (a'|s')q_{\\pi}(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a) =\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)r + \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a) \\gamma \\sum_{a' \\in A(s)}\\pi (a'|s')q_{\\pi}(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a) =\\sum_{r\\in R}r\\sum_{s'\\in S}p(s',r|s,a) +  \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)\\gamma\\sum_{a' \\in A(s)}\\pi (a'|s')q_{\\pi}(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we substiture (3.4) and (3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a) =r(s,a) +  \\sum_{s'\\in S}p(s'|s,a)\\gamma\\sum_{a' \\in A(s)}\\pi (a'|s')q_{\\pi}(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $v_*(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}(s) = \\max_{a\\in A(s)}\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma v_{*}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}(s) = \\max_{a\\in A(s)}(\\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)r + \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)\\gamma v_{*}(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}(s) = \\max_{a\\in A(s)}(\\sum_{r\\in R}r\\sum_{s'\\in S}p(s',r|s,a) + \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)\\gamma v_{*}(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we substiture (3.4) and (3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}(s) = \\max_{a\\in A(s)}(r(s,a) + \\sum_{s'\\in S}p(s'|s,a)\\gamma v_{*}(s'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{*}(s,a) = \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)[r + \\gamma\\max_{a'\\in A(s')} q_{*}(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{*}(s,a) = \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)r + \\sum_{s'\\in S}\\sum_{r\\in R}\\gamma\\max_{a'\\in A(s')} q_{*}(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{*}(s,a) = \\sum_{r\\in R}r\\sum_{s'\\in S}p(s',r|s,a) + \\sum_{s'\\in S}\\sum_{r\\in R}p(s',r|s,a)\\gamma\\max_{a'\\in A(s')} q_{*}(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we substiture (3.4) and (3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{*}(s,a) = r(s,a) + \\sum_{s'\\in S}p(s'|s,a)\\gamma\\max_{a'\\in A(s')} q_{*}(s',a')$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
