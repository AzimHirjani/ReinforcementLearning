{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Theory: Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is a simple problem, it cannot readily be solved in a satisfactory way\n",
    "through classical techniques. For example, the classical “minimax” solution from game\n",
    "theory is not correct here because it assumes a particular way of playing by the opponent.\n",
    "For example, a minimax player would never reach a game state from which it could\n",
    "lose, even if in fact it always won from that state because of incorrect play by the\n",
    "opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Minimax is an algorithm that tries to minimize your opponent's maximum score using backward induction. Minimax assumes that the opponent is always selecting the move that maximizes their result and is actively trying to win. The minimax solution is not correct because, in this case, we have assumed that the opponent can make mistakes, so they might not be maximizing their score when they do. Since we are also selecting the move that minimizes the opponent's score, we would not reach a state where we can lose and then win because the opponent made a mistake. The video below will explain the Minimax algorithm.\n",
    "\n",
    "https://www.youtube.com/watch?v=uzsnQ1a9fq0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 Self-Play "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it would learn a different policy for selecting moves. Instead of playing against a static opponent with a specific playstyle, we will play ourselves, who is slowly changing its way of playing through learning and updating our state values. We are also exploring, which means we will experience more unique games and states. As previously mentioned, we will learn to play optimally against our opponent; since our opponents are different, we will learn to play optimally for them, meaning our policy is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 Symmetries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tic Tac Toe board has three axes of symmetry, and we can reduce the total amount of states and state values that we need to track by a factor of 8. Symmetry would improve the learning process by making learning quicker and reducing the memory required. If our opponent does not take advantage of symmetries, we should not because if we are playing against an imperfect opponent, we can miss taking advantage of these imperfect plays. For example, let us say at the start of a tic tac toe game, we place an X in the top right corner. This is symmetrically equivalent to placing an X in each corner of the board. Now let us say that every time at the beginning of a game we place an X in the top right corner the opponent is guaranteed to place an O in the bottom right but the opponent plays perfectly in all other situations including if we put an X in the top left corner which is symmetrically equivalent to X in the top right. With symmetry, this imperfect play would not be adequately learned, and we would not be able to capitalize on this specific incorrect play because, with symmetry, we would not be able to differentiate the specific corner the X was placed. This is why all symmetrically equivalent positions should not necessarily have the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3 Greedy Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A greedy reinforcement learning player would play learn to play worse than a non-greedy player. A problem that will occur with a greedy learner is that it would not explore and learn about possible better moves causing it to play the same less than optimal moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4 Learning from Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do not learn from exploratory moves, we learn the probabilities of winning from each state if we played optimally from then on. When we learn from exploratory moves, we learn the probabilities of winning from each state if we played optimally from them on but also taking into account the exploratory moves. Assuming we do continue to make exploratory moves after we converge, the set of probabilities that take into account exploratory moves would result in more wins because it is prepared for us to act less than optimal and can try to get the best outcome even though we acted this way. The set of probabilities of winning from each state if we played optimally would not take into account this nonoptimal exploratory move and might not know how to win from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1.5 Other Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the reinforcement learning player is to allow it to go second."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
